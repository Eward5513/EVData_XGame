{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Auto-generated from `process_try_to_flow.py`\n\nGenerated on 2025-11-09T22:01:34.\n\nThis notebook was created programmatically to mirror the original Python script.\n"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": "from pathlib import Path\nimport sys\nproject_root = str(Path.cwd().parent.resolve())\nif project_root not in sys.path:\n    sys.path.insert(0, project_root)\n"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\"\"\"\nBuild per-second flow from data/cross_over_time_try.csv (A0003 only) and\nthen fold into a single signal cycle by summing corresponding seconds.\n\nOutputs:\n  - data/per_second_flow.csv\n  - data/per_cycle_flow.csv\n\"\"\"\nimport os\nfrom typing import Dict, List, Optional, Tuple\n\nimport pandas as pd\n\n\nDIR4: Tuple[str, str, str, str] = (\"A1\", \"A2\", \"B1\", \"B2\")\n\n# Fixed configuration\nTRY_CSV = \"/home/mw/project/cross_over_time_try.csv\"\nOUT_PER_SECOND_FLOW = \"/home/mw/project/per_second_flow.csv\"\nOUT_PER_CYCLE_FLOW = \"/home/mw/project/per_cycle_flow.csv\"\nROAD_ID = \"A0003\"\nSTART_TIME_STR = \"16:34:24\"\nEND_TIME_STR = \"17:28:33\"\nCYCLE_SECONDS = 130\n\n\ndef _to_dir4(direction: object) -> Optional[str]:\n    try:\n        d = str(direction)\n    except Exception:\n        return None\n    for k in DIR4:\n        if d.startswith(k):\n            return k\n    return None\n\n\ndef _read_try_csv(path: str) -> pd.DataFrame:\n    if not os.path.exists(path):\n        raise FileNotFoundError(f\"File not found: {path}\")\n    df = pd.read_csv(path)\n    need = {\"road_id\", \"date\", \"direction\", \"enter_time\", \"exit_time\"}\n    if not need.issubset(set(df.columns)):\n        missing = sorted(list(need - set(df.columns)))\n        raise ValueError(f\"Missing required columns in {path}: {missing}\")\n    return df\n\n\ndef build_per_second_flow_from_try(trdf: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    Per-second occupancy counts for A1/A2/B1/B2.\n    Interval is [enter_time, exit_time), i.e., end exclusive.\n    Returns columns: road_id, date, time, A1, A2, B1, B2\n    \"\"\"\n    if trdf is None or trdf.empty:\n        return pd.DataFrame(columns=[\"road_id\", \"date\", \"time\", *DIR4])\n    work = trdf.copy()\n    # Filter to one road\n    work = work[work[\"road_id\"] == ROAD_ID].copy()\n    if work.empty:\n        return pd.DataFrame(columns=[\"road_id\", \"date\", \"time\", *DIR4])\n    # Map to 4-direction groups\n    work[\"dir4\"] = work[\"direction\"].map(_to_dir4)\n    work = work.dropna(subset=[\"dir4\"]).astype({\"dir4\": str})\n    # Build absolute timestamps\n    work[\"start\"] = pd.to_datetime(work[\"date\"].astype(str) + \" \" + work[\"enter_time\"].astype(str), errors=\"coerce\")\n    work[\"end\"] = pd.to_datetime(work[\"date\"].astype(str) + \" \" + work[\"exit_time\"].astype(str), errors=\"coerce\")\n    # Filter valid, non-zero intervals\n    work = work[(~work[\"start\"].isna()) & (~work[\"end\"].isna()) & (work[\"start\"] < work[\"end\"])].copy()\n    if work.empty:\n        return pd.DataFrame(columns=[\"road_id\", \"date\", \"time\", *DIR4])\n\n    out_frames: List[pd.DataFrame] = []\n    for (road_id, date_val), g in work.groupby([\"road_id\", \"date\"]):\n        # Collect event deltas per direction\n        dir_events: Dict[str, Dict[pd.Timestamp, int]] = {d: {} for d in DIR4}\n        for _, r in g.iterrows():\n            d = r[\"dir4\"]\n            s = r[\"start\"]\n            e = r[\"end\"]\n            dir_events[d][s] = dir_events[d].get(s, 0) + 1\n            dir_events[d][e] = dir_events[d].get(e, 0) - 1\n        # Determine group time span\n        all_keys = []\n        for d in DIR4:\n            all_keys.extend(list(dir_events[d].keys()))\n        if not all_keys:\n            continue\n        t0 = min(all_keys).floor(\"s\")\n        t1 = max(all_keys).ceil(\"s\")\n        if t0 >= t1:\n            continue\n        # Build 1-second index; end exclusive\n        idx = pd.date_range(t0, t1 - pd.Timedelta(seconds=1), freq=\"s\")\n        data = {}\n        for d in DIR4:\n            ev = dir_events[d]\n            if not ev:\n                series = pd.Series(0, index=idx, dtype=\"int64\")\n            else:\n                evs = pd.Series(ev).sort_index()\n                series = evs.cumsum()\n                # Align to idx: forward fill current occupancy and fill gaps with 0\n                series = series.reindex(idx, method=\"ffill\").fillna(0).astype(\"int64\")\n            data[d] = series\n        df_group = pd.DataFrame(data, index=idx).reset_index().rename(columns={\"index\": \"dt\"})\n        df_group[\"road_id\"] = road_id\n        df_group[\"date\"] = date_val\n        df_group[\"time\"] = df_group[\"dt\"].dt.strftime(\"%H:%M:%S\")\n        out_frames.append(df_group[[\"road_id\", \"date\", \"time\", *DIR4]])\n    if not out_frames:\n        return pd.DataFrame(columns=[\"road_id\", \"date\", \"time\", *DIR4])\n    out = pd.concat(out_frames, ignore_index=True)\n    # Ensure integer dtype\n    for d in DIR4:\n        out[d] = out[d].astype(\"int64\")\n    return out.sort_values([\"road_id\", \"date\", \"time\"]).reset_index(drop=True)\n\n\ndef _fold_per_second_to_cycle(per_second_df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    Fold A0003 per-second flow into one 130s cycle across all dates\n    within [START_TIME_STR, END_TIME_STR].\n    \"\"\"\n    if per_second_df is None or per_second_df.empty:\n        return pd.DataFrame(columns=[\"offset_s\", \"time\", *DIR4])\n    work = per_second_df[per_second_df[\"road_id\"] == ROAD_ID].copy()\n    if work.empty:\n        return pd.DataFrame(columns=[\"offset_s\", \"time\", *DIR4])\n    # Time-of-day as dummy dt for range operations\n    work[\"dt\"] = pd.to_datetime(\"1970-01-01 \" + work[\"time\"].astype(str), errors=\"coerce\")\n    start_dt = pd.to_datetime(\"1970-01-01 \" + START_TIME_STR)\n    end_dt = pd.to_datetime(\"1970-01-01 \" + END_TIME_STR)\n    # Keep only window\n    work = work[(work[\"dt\"] >= start_dt) & (work[\"dt\"] <= end_dt)].copy()\n    if work.empty:\n        return pd.DataFrame(columns=[\"offset_s\", \"time\", *DIR4])\n    # Sum across all dates for each dt second\n    summed = work.groupby(\"dt\", as_index=False)[list(DIR4)].sum()\n    summed = summed.set_index(\"dt\")\n    # Ensure complete 1s coverage\n    full_idx = pd.date_range(start=start_dt, end=end_dt, freq=\"s\")\n    summed = summed.reindex(full_idx).fillna(0.0)\n    # Compute cycle offset\n    offsets = ((summed.index - start_dt) / pd.Timedelta(seconds=1)).astype(\"int64\") % CYCLE_SECONDS\n    summed = summed.copy()\n    summed[\"offset_s\"] = offsets\n    folded = summed.groupby(\"offset_s\", as_index=False)[list(DIR4)].sum()\n    folded[\"time\"] = (start_dt + pd.to_timedelta(folded[\"offset_s\"], unit=\"s\")).dt.strftime(\"%H:%M:%S\")\n    folded = folded.sort_values(\"offset_s\")[[\"offset_s\", \"time\", *DIR4]].reset_index(drop=True)\n    for d in DIR4:\n        folded[d] = folded[d].round().astype(\"int64\")\n    return folded\n\n\ndef main() -> int:\n    trdf = _read_try_csv(TRY_CSV)\n    per_second_flow = build_per_second_flow_from_try(trdf)\n    if per_second_flow is None or per_second_flow.empty:\n        # Still write an empty CSV with expected columns\n        pd.DataFrame(columns=[\"road_id\", \"date\", \"time\", *DIR4]).to_csv(OUT_PER_SECOND_FLOW, index=False)\n        print(f\"No rows; wrote empty {OUT_PER_SECOND_FLOW}\")\n        # And an empty per-cycle CSV\n        pd.DataFrame(columns=[\"offset_s\", \"time\", *DIR4]).to_csv(OUT_PER_CYCLE_FLOW, index=False)\n        print(f\"No rows; wrote empty {OUT_PER_CYCLE_FLOW}\")\n        return 0\n    per_second_flow.to_csv(OUT_PER_SECOND_FLOW, index=False)\n    print(f\"Wrote {len(per_second_flow)} per-second rows to {OUT_PER_SECOND_FLOW}\")\n    per_cycle_flow = _fold_per_second_to_cycle(per_second_flow)\n    per_cycle_flow.to_csv(OUT_PER_CYCLE_FLOW, index=False)\n    print(f\"Wrote {len(per_cycle_flow)} per-cycle rows to {OUT_PER_CYCLE_FLOW}\")\n    return 0\n\n\nif __name__ == \"__main__\":\n    raise SystemExit(main())\n\n\n"
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}