{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Auto-generated from `infer_timing.py`\n\nGenerated on 2025-11-09T22:01:34.\n\nThis notebook was created programmatically to mirror the original Python script.\n"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": "from pathlib import Path\nimport sys\nproject_root = str(Path.cwd().parent.resolve())\nif project_root not in sys.path:\n    sys.path.insert(0, project_root)\n"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\"\"\"\nDual-anchor cycle estimator (v6)\n- Computes period from GREEN-start spacing and RED-start spacing per group.\n- Reconciles them per group, then aggregates to a single GLOBAL period.\n- Prints ONLY the final global period (integer seconds) to stdout.\n- Saves rich per-group stats for plotting.\n\nIO fixed to /home/tzhang174/EVData_XGame/data\nInputs expected:\n- wait.csv (red intervals), with columns: time_stamp, end_time, road_id, date, seg_id, direction (micro, e.g. A1-1), direction_group (A1/A2)\n- cross_over_time_A.csv (green intervals for A group), with columns: enter_time, exit_time, road_id, date, seg_id, direction (micro), direction_group (A1/A2)\n\"\"\"\n\nfrom dataclasses import dataclass\nfrom datetime import datetime, timedelta\nfrom typing import List, Tuple, Dict, Optional\nimport numpy as np\nimport pandas as pd\nimport os, json, sys\n\n# ---------------- Fixed configuration ----------------\nBASE_DIR = \"/home/tzhang174/EVData_XGame/data\"\nWAIT_FILE = os.path.join(BASE_DIR, \"/home/mw/project/wait.csv\")                       # red\n# Use A-group green intervals exclusively\nCROSS_FILE_A = os.path.join(BASE_DIR, \"/home/mw/project/cross_over_time_A.csv\")       # green (A group only)\n# Real (A1) green intervals (file name kept consistent with generator)\nREAL_CROSS_FILE = os.path.join(BASE_DIR, \"/home/mw/project/corss_timeA_real.csv\")\n# Fallback alternate name if needed\nREAL_CROSS_FILE_FALLBACK = os.path.join(BASE_DIR, \"/home/mw/project/cross_over_timeA_real.csv\")\n# With-outer (union including outer-adjacent fallback intervals)\nCROSS_FILE_WITH_OUTER = os.path.join(BASE_DIR, \"/home/mw/project/cross_over_time_with_outer.csv\")\n# All groups (A/B) unified intervals\nCROSS_FILE_ALL = os.path.join(BASE_DIR, \"/home/mw/project/cross_over_time.csv\")\n# Try file (ONLY outer-adjacent intervals)\nCROSS_FILE_TRY = os.path.join(BASE_DIR, \"/home/mw/project/cross_over_time_try.csv\")\n\nOUT_PER_GROUP_CSV = os.path.join(BASE_DIR, \"/home/mw/project/period_stats_per_group.csv\")\nOUT_PER_GROUP_JSON = os.path.join(BASE_DIR, \"/home/mw/project/period_stats_per_group.json\")\nOUT_GLOBAL_JSON = os.path.join(BASE_DIR, \"/home/mw/project/period_stats_global.json\")\n\nTIME_FMT = \"%H:%M:%S\"\n\n# Controls\nTOL_ABS_S = 6.0            # green/red period agreement tolerance in seconds\nMIN_PAIRS = 3              # minimal pairs to trust spacing median\nIQR_CLAMP = 1.5            # clamp factor for robust median\n# -----------------------------------------------------\n\n@dataclass\nclass Interval:\n    start: float\n    end: float\n\ndef parse_time_to_sec(t: str) -> float:\n    dt = datetime.strptime(t, TIME_FMT)\n    return dt.hour*3600 + dt.minute*60 + dt.second\n\ndef parse_time_to_sec_any(t: str) -> Optional[float]:\n    \"\"\"\n    Parse time-of-day from:\n      - 'HH:MM' or 'HH:MM:SS'\n      - 'YYYY-MM-DD HH:MM:SS[.ffffff]'\n    Returns seconds since midnight or None if unparsable.\n    \"\"\"\n    if t is None:\n        return None\n    s = str(t).strip()\n    if not s or s.lower() == \"nat\":\n        return None\n    # Quick parse for HH:MM or HH:MM:SS\n    try:\n        parts = s.split(\":\")\n        if len(parts) == 2:\n            h, m = int(parts[0]), int(parts[1])\n            return float(h * 3600 + m * 60)\n        if len(parts) >= 3 and \" \" not in parts[0]:\n            h, m = int(parts[0]), int(parts[1])\n            sec_part = parts[2]\n            if \".\" in sec_part:\n                sec_part = sec_part.split(\".\")[0]\n            sec = int(sec_part)\n            return float(h * 3600 + m * 60 + sec)\n    except Exception:\n        pass\n    # Fallback: full timestamp parsing\n    try:\n        dt = pd.to_datetime(s, errors=\"coerce\")\n        if pd.isna(dt):\n            return None\n        return float(dt.hour*3600 + dt.minute*60 + dt.second)\n    except Exception:\n        return None\n\ndef load_wait(path: str) -> pd.DataFrame:\n    df = pd.read_csv(path, dtype=str)\n    df.rename(columns={'time_stamp':'start_time', 'end_time':'end_time'}, inplace=True)\n    for col in [\"road_id\",\"direction\",\"seg_id\",\"date\"]:\n        if col not in df.columns:\n            df[col] = \"NA\"\n    # Ensure direction_group exists (A1/A2) for downstream grouping if desired\n    if \"direction_group\" not in df.columns:\n        up = df[\"direction\"].astype(str).str.upper()\n        df[\"direction_group\"] = up.apply(lambda s: \"A1\" if s.startswith(\"A1\") else (\"A2\" if s.startswith(\"A2\") else \"NA\"))\n    return df\n\ndef load_cross(path: str) -> pd.DataFrame:\n    df = pd.read_csv(path, dtype=str)\n    df.rename(columns={'enter_time':'start_time', 'exit_time':'end_time'}, inplace=True)\n    for col in [\"road_id\",\"direction\",\"seg_id\",\"date\"]:\n        if col not in df.columns:\n            df[col] = \"NA\"\n    if \"direction_group\" not in df.columns:\n        up = df[\"direction\"].astype(str).str.upper()\n        df[\"direction_group\"] = up.apply(lambda s: \"A1\" if s.startswith(\"A1\") else (\"A2\" if s.startswith(\"A2\") else \"NA\"))\n    return df\n\ndef make_group_key(row: pd.Series) -> str:\n    # Prefer grouping by A1/A2 if available; fallback to micro direction\n    dir_val = row.get('direction_group', None)\n    if dir_val is None or str(dir_val).strip() == \"\":\n        dir_val = row.get('direction', 'NA')\n    return f\"{row.get('road_id','NA')}|{dir_val}|{row.get('seg_id','NA')}|{row.get('date','NA')}\"\n\ndef build_intervals(df: pd.DataFrame) -> List[Interval]:\n    out: List[Interval] = []\n    for _, r in df.iterrows():\n        try:\n            s_val = parse_time_to_sec_any(r.get('start_time'))\n            e_val = parse_time_to_sec_any(r.get('end_time'))\n            if s_val is None or e_val is None:\n                continue\n            if e_val > s_val:\n                out.append(Interval(float(s_val), float(e_val)))\n        except Exception:\n            continue\n    if not out:\n        return out\n    out.sort(key=lambda x: x.start)\n    # merge within color\n    merged = [out[0]]\n    for it in out[1:]:\n        last = merged[-1]\n        if it.start <= last.end + 1e-6:\n            last.end = max(last.end, it.end)\n        else:\n            merged.append(Interval(it.start, it.end))\n    return merged\n\ndef robust_median(arr: np.ndarray) -> float:\n    if arr.size == 0:\n        return float('nan')\n    q25 = np.percentile(arr, 25); q75 = np.percentile(arr, 75)\n    iqr = q75 - q25\n    if iqr <= 0:\n        return float(np.median(arr))\n    lo, hi = q25 - IQR_CLAMP*iqr, q75 + IQR_CLAMP*iqr\n    f = arr[(arr>=lo) & (arr<=hi)]\n    if f.size == 0:\n        f = arr\n    return float(np.median(f))\n\ndef spacing_median(starts: List[float]) -> Tuple[float, int]:\n    if len(starts) < 2:\n        return float('nan'), 0\n    s = np.diff(np.array(sorted(starts), dtype=float))\n    return robust_median(s), int(len(s))\n\ndef durations_median(intervals: List[Interval]) -> float:\n    if not intervals:\n        return float('nan')\n    d = np.array([iv.end - iv.start for iv in intervals], dtype=float)\n    return robust_median(d)\n\ndef process_group(wait_g: pd.DataFrame, cross_g: pd.DataFrame, key: str):\n    # Parse direction group from key for output tagging\n    dir_group = \"NA\"\n    try:\n        parts = str(key).split(\"|\")\n        if len(parts) >= 2:\n            dir_group = parts[1]\n    except Exception:\n        pass\n    # Intervals\n    reds = build_intervals(wait_g)\n    greens = build_intervals(cross_g)\n\n    red_starts = [iv.start for iv in reds]\n    green_starts = [iv.start for iv in greens]\n\n    green_period, n_g = spacing_median(green_starts)\n    red_period, n_r = spacing_median(red_starts)\n\n    green_med = durations_median(greens)\n    red_med = durations_median(reds)\n\n    # reconcile\n    reconciled = float('nan'); method = \"none\"; agree = False\n    if n_g >= MIN_PAIRS and np.isfinite(green_period) and n_r >= MIN_PAIRS and np.isfinite(red_period):\n        if abs(green_period - red_period) <= TOL_ABS_S:\n            reconciled = 0.5*(green_period + red_period)\n            method = \"fuse_green_red\"\n            agree = True\n        else:\n            if n_g > n_r:\n                reconciled = green_period; method = \"green_only_disagree\"\n            elif n_r > n_g:\n                reconciled = red_period; method = \"red_only_disagree\"\n            else:\n                # tie: pick the one closer to green_med+red_med as weak prior\n                prior = green_med + red_med if np.isfinite(green_med) and np.isfinite(red_med) else np.nan\n                if np.isfinite(prior):\n                    if abs(green_period - prior) <= abs(red_period - prior):\n                        reconciled = green_period; method = \"green_only_tie_prior\"\n                    else:\n                        reconciled = red_period; method = \"red_only_tie_prior\"\n                else:\n                    reconciled = green_period; method = \"green_only_tie\"\n    elif n_g >= MIN_PAIRS and np.isfinite(green_period):\n        reconciled = green_period; method = \"green_only\"\n    elif n_r >= MIN_PAIRS and np.isfinite(red_period):\n        reconciled = red_period; method = \"red_only\"\n    else:\n        method = \"insufficient_pairs\"\n\n    # weights for global aggregation (prefer groups with more pairs on both sides)\n    weight = min(n_g, n_r) if (n_g>0 and n_r>0) else max(n_g, n_r)\n\n    row = {\n        \"group_key\": key,\n        \"direction_group\": dir_group,\n        \"green_period_s\": green_period,\n        \"red_period_s\": red_period,\n        \"reconciled_period_s\": reconciled,\n        \"pairs_green\": n_g,\n        \"pairs_red\": n_r,\n        \"weight\": weight,\n        \"agree_within_tol\": agree,\n        \"method\": method,\n        \"green_median_dur_s\": green_med,\n        \"red_median_dur_s\": red_med\n    }\n    return row\n\ndef weighted_median(values: np.ndarray, weights: np.ndarray) -> float:\n    if values.size == 0:\n        return float('nan')\n    order = np.argsort(values)\n    v = values[order]; w = weights[order]\n    cum = np.cumsum(w) / np.sum(w) if np.sum(w)>0 else np.cumsum(np.ones_like(w))/len(w)\n    idx = np.searchsorted(cum, 0.5)\n    idx = min(idx, len(v)-1)\n    return float(v[idx])\n\ndef compute_real_period() -> Optional[float]:\n    \"\"\"\n    Compute a global green period using A1 real crossover intervals only.\n    Strategy:\n      - Load REAL_CROSS_FILE (fallback to REAL_CROSS_FILE_FALLBACK)\n      - Group by group_key (road|A1|seg|date) and compute per-group spacing median on starts\n      - Aggregate with weighted median by pair counts; fallback to aggregated starts if needed\n    Returns period in seconds (float) or None if not computable.\n    \"\"\"\n    path = None\n    if os.path.exists(REAL_CROSS_FILE):\n        path = REAL_CROSS_FILE\n    elif os.path.exists(REAL_CROSS_FILE_FALLBACK):\n        path = REAL_CROSS_FILE_FALLBACK\n    if path is None:\n        return None\n    try:\n        df = load_cross(path)\n    except Exception:\n        return None\n    # Force A1-only if direction present\n    try:\n        up = df[\"direction\"].astype(str).str.upper()\n        df = df[up.str.startswith(\"A1\")]\n    except Exception:\n        pass\n    if df.empty:\n        return None\n    df[\"group_key\"] = df.apply(make_group_key, axis=1)\n    per_periods: List[float] = []\n    per_weights: List[int] = []\n    # compute per-group green spacing median and pair counts\n    for key, g in df.groupby(\"group_key\"):\n        greens = build_intervals(g)\n        starts = [iv.start for iv in greens]\n        p, n_pairs = spacing_median(starts)\n        if np.isfinite(p) and n_pairs >= MIN_PAIRS:\n            per_periods.append(float(p))\n            per_weights.append(int(n_pairs))\n    if len(per_periods) > 0:\n        return weighted_median(np.array(per_periods, dtype=float), np.array(per_weights, dtype=float))\n    # Fallback: aggregate all starts\n    greens = build_intervals(df)\n    starts = sorted([iv.start for iv in greens])\n    p, _ = spacing_median(starts)\n    return float(p) if np.isfinite(p) else None\n\ndef compute_period_for_road_from_cross(cross_path: str, road_id: str, restrict_A_group: bool = True) -> Optional[float]:\n    \"\"\"\n    Compute a global period for a specific road_id using the same reconciliation logic,\n    combining red intervals from wait.csv and green intervals from cross_path.\n    If restrict_A_group is True, only use A-group greens (direction starts with 'A').\n    Returns seconds (float) or None if not computable.\n    \"\"\"\n    if not os.path.exists(cross_path) or not os.path.exists(WAIT_FILE):\n        return None\n    try:\n        wait_df = load_wait(WAIT_FILE)\n        wait_df = wait_df[wait_df.get(\"road_id\", \"NA\") == road_id].copy()\n        cross_df = load_cross(cross_path)\n        cross_df = cross_df[cross_df.get(\"road_id\", \"NA\") == road_id].copy()\n        if restrict_A_group:\n            try:\n                up = cross_df[\"direction\"].astype(str).str.upper()\n                cross_df = cross_df[up.str.startswith(\"A\")]\n            except Exception:\n                pass\n        if wait_df.empty or cross_df.empty:\n            return None\n        wait_df[\"group_key\"] = wait_df.apply(make_group_key, axis=1)\n        cross_df[\"group_key\"] = cross_df.apply(make_group_key, axis=1)\n        keys = sorted(set(wait_df[\"group_key\"]).union(set(cross_df[\"group_key\"])))\n        if not keys:\n            return None\n        rows = []\n        for key in keys:\n            w = wait_df[wait_df[\"group_key\"] == key]\n            c = cross_df[cross_df[\"group_key\"] == key]\n            if w.empty and c.empty:\n                continue\n            rows.append(process_group(w, c, key))\n        if not rows:\n            return None\n        df = pd.DataFrame(rows)\n        vals = pd.to_numeric(df[\"reconciled_period_s\"], errors=\"coerce\").to_numpy()\n        wts = pd.to_numeric(df[\"weight\"], errors=\"coerce\").fillna(1.0).to_numpy()\n        vals = vals[np.isfinite(vals)]\n        wts = wts[:len(vals)] if len(wts) != len(vals) else wts\n        period = weighted_median(vals, wts) if vals.size > 0 else float('nan')\n        return float(period) if np.isfinite(period) else None\n    except Exception:\n        return None\n\ndef main():\n    # Load wait (required)\n    try:\n        wait_df = load_wait(WAIT_FILE)\n    except Exception:\n        print(\"NaN\")\n        return\n    # Load cross with fallback\n    cross_df = None\n    try:\n        if os.path.exists(CROSS_FILE_A):\n            cross_df = load_cross(CROSS_FILE_A)\n        else:\n            print(\"NaN\")\n            return\n    except Exception:\n        print(\"NaN\")\n        return\n\n    wait_df[\"group_key\"] = wait_df.apply(make_group_key, axis=1)\n    cross_df[\"group_key\"] = cross_df.apply(make_group_key, axis=1)\n\n    keys = sorted(set(wait_df[\"group_key\"]).union(set(cross_df[\"group_key\"])))\n\n    rows = []\n    for key in keys:\n        w = wait_df[wait_df[\"group_key\"] == key]\n        c = cross_df[cross_df[\"group_key\"] == key]\n        rows.append(process_group(w, c, key))\n\n    df = pd.DataFrame(rows)\n    df.to_csv(OUT_PER_GROUP_CSV, index=False, float_format=\"%.3f\")\n    with open(OUT_PER_GROUP_JSON, \"w\", encoding=\"utf-8\") as f:\n        json.dump(rows, f, ensure_ascii=False, indent=2)\n\n    # Global period: weighted median over reconciled per-group period\n    vals = pd.to_numeric(df[\"reconciled_period_s\"], errors=\"coerce\").to_numpy()\n    wts = pd.to_numeric(df[\"weight\"], errors=\"coerce\").fillna(1.0).to_numpy()\n    vals = vals[np.isfinite(vals)]; wts = wts[:len(vals)] if len(wts)!=len(vals) else wts\n\n    global_period = weighted_median(vals, wts) if vals.size>0 else float('nan')\n    out = {\n        \"global_period_s\": global_period,\n        \"aggregation\": \"weighted_median over reconciled per-group periods\",\n        \"weights\": \"min(pairs_green, pairs_red) else max(n_g, n_r)\",\n        \"groups_used\": int(len(vals))\n    }\n    with open(OUT_GLOBAL_JSON, \"w\", encoding=\"utf-8\") as f:\n        json.dump(out, f, ensure_ascii=False, indent=2)\n\n    # PRINT ONLY integer seconds to stdout (round to nearest)\n    if np.isfinite(global_period):\n        print(int(round(global_period)))\n    else:\n        print(\"NaN\")\n    # Also compute and print REAL A1 period (second line)\n    try:\n        real_period = compute_real_period()\n        if real_period is None or not np.isfinite(real_period):\n            print(\"NaN\")\n        else:\n            print(int(round(real_period)))\n    except Exception:\n        print(\"NaN\")\n\n    # Third line: compute period using cross_over_time_with_outer.csv (A-group filtered)\n    try:\n        if os.path.exists(CROSS_FILE_WITH_OUTER):\n            cross_outer_df = load_cross(CROSS_FILE_WITH_OUTER)\n            # Filter to A-group if direction present (A1/A2)\n            try:\n                up = cross_outer_df[\"direction\"].astype(str).str.upper()\n                cross_outer_df = cross_outer_df[up.str.startswith(\"A\")]\n            except Exception:\n                pass\n            cross_outer_df[\"group_key\"] = cross_outer_df.apply(make_group_key, axis=1)\n            keys2 = sorted(set(wait_df[\"group_key\"]).union(set(cross_outer_df[\"group_key\"])))\n            rows2 = []\n            for key in keys2:\n                w = wait_df[wait_df[\"group_key\"] == key]\n                c = cross_outer_df[cross_outer_df[\"group_key\"] == key]\n                rows2.append(process_group(w, c, key))\n            df2 = pd.DataFrame(rows2)\n            vals2 = pd.to_numeric(df2[\"reconciled_period_s\"], errors=\"coerce\").to_numpy()\n            wts2 = pd.to_numeric(df2[\"weight\"], errors=\"coerce\").fillna(1.0).to_numpy()\n            vals2 = vals2[np.isfinite(vals2)]; wts2 = wts2[:len(vals2)] if len(wts2)!=len(vals2) else wts2\n            gp2 = weighted_median(vals2, wts2) if vals2.size>0 else float('nan')\n            if np.isfinite(gp2):\n                print(int(round(gp2)))\n            else:\n                print(\"NaN\")\n        else:\n            print(\"NaN\")\n    except Exception:\n        print(\"NaN\")\n\n    # Fourth line: compute period using cross_over_time_try.csv (A-group filtered)\n    try:\n        if os.path.exists(CROSS_FILE_TRY):\n            cross_try_df = load_cross(CROSS_FILE_TRY)\n            try:\n                up = cross_try_df[\"direction\"].astype(str).str.upper()\n                cross_try_df = cross_try_df[up.str.startswith(\"A\")]\n            except Exception:\n                pass\n            cross_try_df[\"group_key\"] = cross_try_df.apply(make_group_key, axis=1)\n            keys3 = sorted(set(wait_df[\"group_key\"]).union(set(cross_try_df[\"group_key\"])))\n            rows3 = []\n            for key in keys3:\n                w = wait_df[wait_df[\"group_key\"] == key]\n                c = cross_try_df[cross_try_df[\"group_key\"] == key]\n                rows3.append(process_group(w, c, key))\n            df3 = pd.DataFrame(rows3)\n            vals3 = pd.to_numeric(df3[\"reconciled_period_s\"], errors=\"coerce\").to_numpy()\n            wts3 = pd.to_numeric(df3[\"weight\"], errors=\"coerce\").fillna(1.0).to_numpy()\n            vals3 = vals3[np.isfinite(vals3)]; wts3 = wts3[:len(vals3)] if len(wts3)!=len(vals3) else wts3\n            gp3 = weighted_median(vals3, wts3) if vals3.size>0 else float('nan')\n            if np.isfinite(gp3):\n                print(int(round(gp3)))\n            else:\n                print(\"NaN\")\n        else:\n            print(\"NaN\")\n    except Exception:\n        print(\"NaN\")\n\n    # Fifth line: A0008 period using cross_over_time.csv (A-group filtered)\n    try:\n        p_a0008 = compute_period_for_road_from_cross(CROSS_FILE_ALL, \"A0008\", restrict_A_group=True)\n        if p_a0008 is None or not np.isfinite(p_a0008):\n            print(\"NaN\")\n        else:\n            print(int(round(p_a0008)))\n    except Exception:\n        print(\"NaN\")\n\nif __name__ == \"__main__\":\n    main()\n"
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}