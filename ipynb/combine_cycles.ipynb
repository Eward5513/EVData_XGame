{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Auto-generated from `combine_cycles.py`\n\nGenerated on 2025-11-09T22:01:34.\n\nThis notebook was created programmatically to mirror the original Python script.\n"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": "from pathlib import Path\nimport sys\nproject_root = str(Path.cwd().parent.resolve())\nif project_root not in sys.path:\n    sys.path.insert(0, project_root)\n"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\"\"\"\nFold per-second flow/wait series into a single signal cycle by summing\ncorresponding seconds across cycles, and generate four per-cycle plots.\n\nFixed settings:\n  - Input files:\n      - data/per_second_flow.csv\n      - data/per_second_wait.csv\n  - Output files:\n      - data/per_cycle_flow.csv\n      - data/per_cycle_wait.csv\n  - Road filter: A0003 only\n  - Cycle start time (HH:MM:SS): 16:34:24\n  - Cycle end time (HH:MM:SS):   17:28:33\n  - Cycle length: 130 seconds\n  - Number of cycles in window: 25\n\nFor each second offset k in [0, 129], this script sums all rows whose\ntime-of-day equals (start_time + k seconds + n * 130 seconds) for n=0..24,\naggregated across all dates for road A0003.\n\"\"\"\nfrom __future__ import annotations\n\nimport os\nfrom typing import List, Tuple\n\nimport pandas as pd\n\nimport matplotlib\n\nmatplotlib.use(\"Agg\")\nimport matplotlib.pyplot as plt  # noqa: E402\n\n\nDIR4: Tuple[str, str, str, str] = (\"A1\", \"A2\", \"B1\", \"B2\")\n\n# Fixed configuration\nFLOW_CSV = os.path.join(\"data\", \"/home/mw/project/per_second_flow.csv\")\nWAIT_CSV = os.path.join(\"data\", \"/home/mw/project/per_second_wait.csv\")\nOUT_FLOW = os.path.join(\"data\", \"/home/mw/project/per_cycle_flow.csv\")\nOUT_WAIT = os.path.join(\"data\", \"/home/mw/project/per_cycle_wait.csv\")\nPLOT_DIR = \"plots\"\nROAD_ID = \"A0003\"\nSTART_TIME_STR = \"16:34:24\"\nEND_TIME_STR = \"17:28:33\"\nCYCLE_SECONDS = 130\nNUM_CYCLES = 25\n\n\ndef _read_csv(path: str) -> pd.DataFrame:\n    if not os.path.exists(path):\n        raise FileNotFoundError(f\"File not found: {path}\")\n    df = pd.read_csv(path)\n    needed = {\"road_id\", \"date\", \"time\", *DIR4}\n    if not needed.issubset(set(df.columns)):\n        missing = sorted(list(needed - set(df.columns)))\n        raise ValueError(f\"Missing required columns in {path}: {missing}\")\n    return df\n\n\ndef _fold_to_cycle(df: pd.DataFrame) -> pd.DataFrame:\n    if df is None or df.empty:\n        return pd.DataFrame(columns=[\"offset_s\", \"time\", *DIR4])\n    # Keep A0003 only\n    work = df.loc[df[\"road_id\"] == ROAD_ID].copy()\n    if work.empty:\n        return pd.DataFrame(columns=[\"offset_s\", \"time\", *DIR4])\n    # Parse time-of-day to a dummy datetime for range operations\n    work[\"dt\"] = pd.to_datetime(\"1970-01-01 \" + work[\"time\"].astype(str), errors=\"coerce\")\n    start_dt = pd.to_datetime(\"1970-01-01 \" + START_TIME_STR)\n    end_dt = pd.to_datetime(\"1970-01-01 \" + END_TIME_STR)\n    # Filter to window (inclusive)\n    work = work[(work[\"dt\"] >= start_dt) & (work[\"dt\"] <= end_dt)].copy()\n    if work.empty:\n        return pd.DataFrame(columns=[\"offset_s\", \"time\", *DIR4])\n    # Sum across all dates for each second-of-day\n    summed = work.groupby(\"dt\", as_index=False)[list(DIR4)].sum()\n    summed = summed.set_index(\"dt\")\n    # Ensure a complete 1-second index across the window; fill gaps with 0\n    full_idx = pd.date_range(start=start_dt, end=end_dt, freq=\"s\")\n    summed = summed.reindex(full_idx).fillna(0.0)\n    # Compute cycle offset for each second\n    offsets = ((summed.index - start_dt) / pd.Timedelta(seconds=1)).astype(\"int64\") % CYCLE_SECONDS\n    summed = summed.copy()\n    summed[\"offset_s\"] = offsets\n    # Fold: sum by offset\n    folded = summed.groupby(\"offset_s\", as_index=False)[list(DIR4)].sum()\n    # Add canonical time label for each offset (as the first-cycle time)\n    folded[\"time\"] = (start_dt + pd.to_timedelta(folded[\"offset_s\"], unit=\"s\")).dt.strftime(\"%H:%M:%S\")\n    # Order columns and sort by offset\n    folded = folded.sort_values(\"offset_s\")[[\"offset_s\", \"time\", *DIR4]].reset_index(drop=True)\n    # Cast to integers where appropriate\n    for d in DIR4:\n        # values can be floats due to fillna; they are counts so cast safely\n        folded[d] = folded[d].round().astype(\"int64\")\n    return folded\n\n\ndef _ensure_output_dir(path: str) -> None:\n    os.makedirs(path, exist_ok=True)\n\n\ndef _plot_cycle_direction(direction: str,\n                          flow_cycle: pd.DataFrame,\n                          wait_cycle: pd.DataFrame,\n                          out_path: str) -> None:\n    # Align by offset_s and plot\n    f = flow_cycle[[\"offset_s\", direction]].rename(columns={direction: \"flow\"}).copy()\n    w = wait_cycle[[\"offset_s\", direction]].rename(columns={direction: \"wait\"}).copy()\n    f[\"flow\"] = pd.to_numeric(f[\"flow\"], errors=\"coerce\").fillna(0.0)\n    w[\"wait\"] = pd.to_numeric(w[\"wait\"], errors=\"coerce\").fillna(0.0)\n    # Z-score normalization per type for THIS direction\n    flow_mean = float(f[\"flow\"].mean())\n    flow_std = float(f[\"flow\"].std(ddof=0))\n    wait_mean = float(w[\"wait\"].mean())\n    wait_std = float(w[\"wait\"].std(ddof=0))\n    f[\"flow_z\"] = (f[\"flow\"] - flow_mean) / flow_std if flow_std > 0 else 0.0\n    w[\"wait_z\"] = (w[\"wait\"] - wait_mean) / wait_std if wait_std > 0 else 0.0\n    # Merge for plotting\n    merged = pd.merge(f[[\"offset_s\", \"flow_z\"]], w[[\"offset_s\", \"wait_z\"]], on=\"offset_s\", how=\"outer\").fillna(0.0)\n    merged = merged.sort_values(\"offset_s\")\n    plt.figure(figsize=(12, 5))\n    plt.plot(merged[\"offset_s\"], merged[\"flow_z\"], label=\"Flow (z)\", linewidth=1.6)\n    plt.plot(merged[\"offset_s\"], merged[\"wait_z\"], label=\"Wait (z)\", linewidth=1.6)\n    plt.title(f\"{direction} (cycle length {CYCLE_SECONDS}s, N={NUM_CYCLES} cycles)\")\n    plt.xlabel(\"Second in cycle (s)\")\n    plt.ylabel(\"Z-score\")\n    plt.grid(True, alpha=0.3)\n    plt.legend()\n    plt.tight_layout()\n    plt.savefig(out_path, dpi=150)\n    plt.close()\n\n\ndef main() -> int:\n    flow_df = _read_csv(FLOW_CSV)\n    wait_df = _read_csv(WAIT_CSV)\n    flow_cycle = _fold_to_cycle(flow_df)\n    wait_cycle = _fold_to_cycle(wait_df)\n    flow_cycle.to_csv(OUT_FLOW, index=False)\n    wait_cycle.to_csv(OUT_WAIT, index=False)\n    print(f\"Wrote {len(flow_cycle)} rows to {OUT_FLOW}\")\n    print(f\"Wrote {len(wait_cycle)} rows to {OUT_WAIT}\")\n    # Plots\n    _ensure_output_dir(PLOT_DIR)\n    out_paths: List[str] = []\n    for d in DIR4:\n        out = os.path.join(PLOT_DIR, f\"/home/mw/project/per_cycle_{d}.png\")\n        _plot_cycle_direction(d, flow_cycle, wait_cycle, out)\n        out_paths.append(out)\n    print(f\"Wrote {len(out_paths)} figures:\")\n    for p in out_paths:\n        print(f\" - {p}\")\n    return 0\n\n\nif __name__ == \"__main__\":\n    raise SystemExit(main())\n\n\n"
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}