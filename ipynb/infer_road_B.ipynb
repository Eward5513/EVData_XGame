{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Auto-generated from `infer_road_B.py`\n\nGenerated on 2025-11-09T22:01:34.\n\nThis notebook was created programmatically to mirror the original Python script.\n"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": "from pathlib import Path\nimport sys\nproject_root = str(Path.cwd().parent.resolve())\nif project_root not in sys.path:\n    sys.path.insert(0, project_root)\n"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": "\n# -*- coding: utf-8 -*-\n\"\"\"\nB 向（南北向）车道提取与分析双输出：\n- 仅使用方向 B1-1 / B1-2 的轨迹\n- 计算分界线（PCA-1D + 沿程分箱 17m + t 轴双峰）\n- 侧别按东西（EAST/WEST），用 +1.75m 偏移的经度差确定 “+” 是东还是西\n- 生成两文件：\n  1) intersection_B.json：分界线 + 东/西各两条直行车道中轴（共 4 条），偏移为 1.75/5.25m\n  2) analyse_B.json：统计信息（|t| 直方图、峰位置、推断车道数、参数）, 便于后续画图\n\n数据与输出目录（固定）：/home/tzhang174/EVData_XGame/data/\n\"\"\"\n\nimport os, csv, json, math\nfrom typing import Any, Dict, List, Tuple\nimport numpy as np\n\n# ---------------- Config ----------------\nDATA_DIR      = \"/home/tzhang174/EVData_XGame/data\"\nINPUT_FILES   = [(\"/home/mw/project/A0003_refined.csv\", \"A0003\"), (\"/home/mw/project/A0008_refined.csv\", \"A0008\")]\nOUT_INTER     = os.path.join(DATA_DIR, \"/home/mw/project/intersection_B.json\")\nOUT_ANALYSE   = os.path.join(DATA_DIR, \"/home/mw/project/analyse_B.json\")\n\nBIN_SIZE_M        = 17.0\nMIN_PTS_PER_BIN   = 25\nNBINS_T           = 64\nMIN_LANE_SEP_M    = 3.2\nSIGMA_BINS        = 1.5\nSMOOTH_WIN        = 7\nLANE_WIDTH        = 3.5\nHALF              = LANE_WIDTH / 2.0  # 1.75 m\n\n# ---------------- Geometry helpers ----------------\ndef lonlat_to_local_xy(lon, lat, lon0=None, lat0=None):\n    lon = np.asarray(lon, float); lat = np.asarray(lat, float)\n    if lon0 is None: lon0 = float(np.nanmean(lon))\n    if lat0 is None: lat0 = float(np.nanmean(lat))\n    R = 6371000.0\n    x = R*np.cos(np.deg2rad(lat0))*np.deg2rad(lon - lon0)\n    y = R*np.deg2rad(lat - lat0)\n    return x, y, lon0, lat0\n\ndef local_xy_to_lonlat(x, y, lon0, lat0):\n    R = 6371000.0\n    lon = lon0 + np.rad2deg(x/(R*np.cos(np.deg2rad(lat0))))\n    lat = lat0 + np.rad2deg(y/R)\n    return lon, lat\n\ndef pca_first_axis(X: np.ndarray):\n    mu = X.mean(axis=0)\n    Y  = X - mu\n    C  = (Y.T @ Y) / max(len(X)-1, 1)\n    w, V = np.linalg.eigh(C)\n    u = V[:, np.argmax(w)]; u /= np.linalg.norm(u)\n    v = np.array([-u[1], u[0]])\n    return u, v, mu\n\n# ---------------- Peak helpers ----------------\ndef _gauss_smooth_1d(y: np.ndarray, sigma_bins=1.5, radius=4):\n    if len(y) == 0: return y\n    r = int(max(1, np.ceil(radius*sigma_bins)))\n    xs = np.arange(-r, r+1)\n    ker = np.exp(-0.5*(xs/sigma_bins)**2); ker /= ker.sum()\n    pad = (len(ker)-1)//2\n    yp  = np.pad(y, (pad,pad), mode='edge')\n    return np.convolve(yp, ker, mode='valid')\n\ndef _two_peaks_in_bin(tvals: np.ndarray, nbins=64, min_sep=3.2, sigma_bins=1.5):\n    if len(tvals) < 5: return None\n    tmin, tmax = np.quantile(tvals, 0.02), np.quantile(tvals, 0.98)\n    if not np.isfinite(tmin) or not np.isfinite(tmax): return None\n    if tmax <= tmin: tmax = tmin + 1e-3\n    H, edges = np.histogram(tvals, bins=nbins, range=(tmin, tmax))\n    Hs = _gauss_smooth_1d(H, sigma_bins=sigma_bins)\n    peaks = []\n    for i in range(1, len(Hs)-1):\n        if Hs[i] >= Hs[i-1] and Hs[i] >= Hs[i+1] and Hs[i] > 0:\n            t_center = 0.5*(edges[i] + edges[i+1])\n            peaks.append((Hs[i], t_center))\n    if not peaks: return None\n    peaks.sort(reverse=True, key=lambda z: z[0])\n    top = []\n    for h, tc in peaks:\n        if not top or all(abs(tc - tc2) >= min_sep for _, tc2 in top):\n            top.append((h, tc))\n        if len(top) == 2: break\n    if len(top) == 1: return (top[0][1], None)\n    t1 = min(top[0][1], top[1][1]); t2 = max(top[0][1], top[1][1])\n    return (t1, t2)\n\ndef _k_peaks_1d(values: np.ndarray, nbins=64, min_sep=3.2, sigma_bins=1.5, kmax=4, min_rel_height=0.08):\n    if values.size < 5: return []\n    vmin, vmax = np.quantile(values, 0.02), np.quantile(values, 0.98)\n    if not np.isfinite(vmin) or not np.isfinite(vmax): return []\n    if vmax <= vmin: vmax = vmin + 1e-3\n    H, edges = np.histogram(values, bins=nbins, range=(vmin, vmax))\n    Hs = _gauss_smooth_1d(H, sigma_bins=sigma_bins)\n    Hmax = float(np.max(Hs)) if Hs.size>0 else 0.0\n    if Hmax <= 0: return []\n    peaks = []\n    for i in range(1, len(Hs)-1):\n        if Hs[i] >= Hs[i-1] and Hs[i] >= Hs[i+1] and Hs[i] >= min_rel_height*Hmax:\n            t_center = 0.5*(edges[i] + edges[i+1])\n            peaks.append((Hs[i], t_center))\n    if not peaks: return []\n    peaks.sort(reverse=True, key=lambda z: z[0])\n    selected = []\n    for h, tc in peaks:\n        if all(abs(tc - s) >= min_sep for s in selected):\n            selected.append(tc)\n        if len(selected) >= kmax: break\n    selected.sort()\n    return selected\n\n# ---------------- Divider ----------------\ndef _lane_divider_st_and_xy(s: np.ndarray, t: np.ndarray, u: np.ndarray, v: np.ndarray, mu: np.ndarray):\n    bin_size_m=BIN_SIZE_M; min_pts_per_bin=MIN_PTS_PER_BIN; nbins_t=NBINS_T\n    min_lane_sep_m=MIN_LANE_SEP_M; sigma_bins=SIGMA_BINS; smooth_win=SMOOTH_WIN\n    if s.size == 0: return np.array([]), np.array([]), np.array([]), np.array([])\n    smin, smax = float(np.min(s)), float(np.max(s))\n    if not np.isfinite(smin) or not np.isfinite(smax) or smax - smin < 1e-9:\n        return np.array([]), np.array([]), np.array([]), np.array([])\n    edges = np.arange(smin, smax + bin_size_m, bin_size_m)\n    idx   = np.digitize(s, edges) - 1\n    T_div_bins = np.full(len(edges)-1, np.nan)\n    for b in range(len(edges)-1):\n        m = (idx == b)\n        if np.sum(m) < min_pts_per_bin: continue\n        tp = _two_peaks_in_bin(t[m], nbins=nbins_t, min_sep=min_lane_sep_m, sigma_bins=sigma_bins)\n        if tp is None: continue\n        t1, t2 = tp\n        t_div = t1 if t2 is None else 0.5*(t1 + t2)\n        T_div_bins[b] = float(t_div)\n    if np.all(np.isnan(T_div_bins)): return np.array([]), np.array([]), np.array([]), np.array([])\n    idx_bins = np.arange(len(T_div_bins)); good = ~np.isnan(T_div_bins)\n    if np.sum(good) >= 2: T_div_bins = np.interp(idx_bins, idx_bins[good], T_div_bins[good])\n    elif np.sum(good) == 1: T_div_bins[:] = T_div_bins[good][0]\n    S_centers = 0.5*(edges[:-1] + edges[1:])\n    T_arr = T_div_bins.copy()\n    if smooth_win and smooth_win > 1:\n        if smooth_win % 2 == 0: smooth_win += 1\n        pad = smooth_win//2; ker = np.ones(smooth_win)/smooth_win\n        Tp  = np.pad(T_arr, (pad,pad), mode='edge')\n        T_arr = np.convolve(Tp, ker, mode='valid')\n    C = mu + S_centers[:,None]*u + T_arr[:,None]*v\n    cx, cy = C[:,0], C[:,1]\n    return S_centers, T_arr, cx, cy\n\n# ---------------- IO ----------------\ndef load_directions(path: str) -> set:\n    allowed = set()\n    with open(path, \"r\", newline=\"\", encoding=\"utf-8\") as f:\n        reader = csv.DictReader(f)\n        for row in reader:\n            d = row.get(\"direction\",\"\").strip()\n            if d in {\"B1-1\",\"B1-2\"}:\n                key = (\n                    row.get(\"vehicle_id\",\"\").strip(),\n                    row.get(\"date\",\"\").strip(),\n                    row.get(\"seg_id\",\"\").strip(),\n                    row.get(\"road_id\",\"\").strip(),\n                )\n                allowed.add(key)\n    return allowed\n\ndef load_points(path: str, allowed: set, expect_road: str):\n    lons, lats = [], []\n    with open(path, \"r\", newline=\"\", encoding=\"utf-8\") as f:\n        reader = csv.DictReader(f)\n        for row in reader:\n            if row.get(\"road_id\",\"\").strip() != expect_road: continue\n            key = (\n                row.get(\"vehicle_id\",\"\").strip(),\n                row.get(\"date\",\"\").strip(),\n                row.get(\"seg_id\",\"\").strip(),\n                row.get(\"road_id\",\"\").strip(),\n            )\n            if key not in allowed: continue\n            try:\n                lon = float(row.get(\"longitude\",\"\")); lat = float(row.get(\"latitude\",\"\"))\n            except ValueError:\n                continue\n            if math.isfinite(lon) and math.isfinite(lat):\n                lons.append(lon); lats.append(lat)\n    return np.array(lons, float), np.array(lats, float)\n\n# ---------------- Normals & Offsets ----------------\ndef _compute_normals(cx: np.ndarray, cy: np.ndarray):\n    n = len(cx)\n    if n < 2: return np.array([]), np.array([])\n    tx = np.zeros(n); ty = np.zeros(n)\n    dx = np.diff(cx); dy = np.diff(cy)\n    tx[0], ty[0] = dx[0], dy[0]\n    tx[-1], ty[-1] = dx[-1], dy[-1]\n    if n > 2:\n        tx[1:-1] = (cx[2:] - cx[:-2]) * 0.5\n        ty[1:-1] = (cy[2:] - cy[:-2]) * 0.5\n    L = np.hypot(tx, ty); L[L==0]=1.0\n    tx/=L; ty/=L\n    nx, ny = -ty, tx\n    NL = np.hypot(nx, ny); NL[NL==0]=1.0\n    nx/=NL; ny/=NL\n    return nx, ny\n\ndef _offset_polyline(cx: np.ndarray, cy: np.ndarray, d: float):\n    nx, ny = _compute_normals(cx, cy)\n    if nx.size == 0: return np.array([]), np.array([])\n    return cx + d*nx, cy + d*ny\n\n# ---------------- Main per-road ----------------\ndef process_one(points_csv: str, road_id: str, allowed: set):\n    lons, lats = load_points(points_csv, allowed, road_id)\n    if lons.size < 10:\n        return (\n            {\n                \"lane_divider\": [],\n                \"east\": {\"lanes\": {\"east_lane1\": [], \"east_lane2\": []}},\n                \"west\": {\"lanes\": {\"west_lane1\": [], \"west_lane2\": []}}\n            },\n            {\n                \"all\":   {\"abs_t_hist_edges_m\": [], \"abs_t_hist_counts\": []},\n                \"east\":  {\"abs_t_hist_edges_m\": [], \"abs_t_hist_counts\": [], \"abs_t_peaks_m\": [], \"likely_lane_count\": 0},\n                \"west\":  {\"abs_t_hist_edges_m\": [], \"abs_t_hist_counts\": [], \"abs_t_peaks_m\": [], \"likely_lane_count\": 0},\n                \"params\": params_dict()\n            }\n        )\n\n    x, y, lon0, lat0 = lonlat_to_local_xy(lons, lats)\n    X  = np.column_stack([x, y])\n    u, v, mu = pca_first_axis(X)\n    Y  = X - mu\n    s  = Y @ u\n    t  = Y @ v\n\n    # Divider in s,t and XY\n    S_centers, T_div_s, cx, cy = _lane_divider_st_and_xy(s, t, u, v, mu)\n    if cx.size < 2:\n        return (\n            {\n                \"lane_divider\": [],\n                \"east\": {\"lanes\": {\"east_lane1\": [], \"east_lane2\": []}},\n                \"west\": {\"lanes\": {\"west_lane1\": [], \"west_lane2\": []}}\n            },\n            {\n                \"all\":   {\"abs_t_hist_edges_m\": [], \"abs_t_hist_counts\": []},\n                \"east\":  {\"abs_t_hist_edges_m\": [], \"abs_t_hist_counts\": [], \"abs_t_peaks_m\": [], \"likely_lane_count\": 0},\n                \"west\":  {\"abs_t_hist_edges_m\": [], \"abs_t_hist_counts\": [], \"abs_t_peaks_m\": [], \"likely_lane_count\": 0},\n                \"params\": params_dict()\n            }\n        )\n\n    # EAST/WEST mapping by longitude using +1.75 m test offset\n    ox_test, oy_test = _offset_polyline(cx, cy, HALF)\n    test_lon, _ = local_xy_to_lonlat(ox_test, oy_test, lon0, lat0)\n    div_lon,  _ = local_xy_to_lonlat(cx, cy, lon0, lat0)\n    import numpy as _np\n    east_sign = +1.0 if _np.median(test_lon - div_lon) > 0.0 else -1.0\n\n    # Interpolate divider t(s) for each point, compute lateral offset\n    T_interp = np.interp(s, S_centers, T_div_s)\n    t_rel = t - T_interp\n\n    # EAST if east_sign * t_rel >= 0 ; WEST otherwise\n    east_mask = (east_sign * t_rel) >= 0.0\n    west_mask = ~east_mask\n\n    abs_t_all  = np.abs(t_rel)\n    abs_t_east = np.abs(t_rel[east_mask])\n    abs_t_west = np.abs(t_rel[west_mask])\n\n    def _hist_pack(vals: np.ndarray, nbins=60):\n        if vals.size == 0: return [], []\n        vmax = float(np.quantile(vals, 0.995))\n        if vmax <= 0: vmax = float(np.max(vals)) if vals.size>0 else 1.0\n        if vmax <= 0: vmax = 1.0\n        H, edges = np.histogram(vals, bins=nbins, range=(0.0, vmax))\n        return edges.tolist(), H.tolist()\n\n    all_edges,  all_counts   = _hist_pack(abs_t_all)\n    east_edges, east_counts  = _hist_pack(abs_t_east)\n    west_edges, west_counts  = _hist_pack(abs_t_west)\n\n    east_peaks = _k_peaks_1d(abs_t_east, nbins=64, min_sep=LANE_WIDTH*0.9, sigma_bins=1.5, kmax=4, min_rel_height=0.08)\n    west_peaks = _k_peaks_1d(abs_t_west, nbins=64, min_sep=LANE_WIDTH*0.9, sigma_bins=1.5, kmax=4, min_rel_height=0.08)\n\n    # --- Build intersection_B.json content ---\n    divider_lon, divider_lat = local_xy_to_lonlat(cx, cy, lon0, lat0)\n    inter = {\n        \"lane_divider\": [[float(a), float(b)] for a,b in zip(divider_lon.tolist(), divider_lat.tolist())],\n        \"east\": {\"lanes\": {}},\n        \"west\": {\"lanes\": {}}\n    }\n    east_offsets = [ east_sign*(HALF), east_sign*(HALF + LANE_WIDTH) ]      # +1.75, +5.25 toward EAST\n    west_offsets = [ -east_sign*(HALF), -east_sign*(HALF + LANE_WIDTH) ]    # -1.75, -5.25 toward WEST\n\n    for i, d in enumerate(east_offsets, start=1):\n        ox, oy = _offset_polyline(cx, cy, d)\n        lon, lat = local_xy_to_lonlat(ox, oy, lon0, lat0)\n        inter[\"east\"][\"lanes\"][f\"east_lane{i}\"] = [[float(a), float(b)] for a,b in zip(lon.tolist(), lat.tolist())]\n    for i, d in enumerate(west_offsets, start=1):\n        ox, oy = _offset_polyline(cx, cy, d)\n        lon, lat = local_xy_to_lonlat(ox, oy, lon0, lat0)\n        inter[\"west\"][\"lanes\"][f\"west_lane{i}\"] = [[float(a), float(b)] for a,b in zip(lon.tolist(), lat.tolist())]\n\n    # --- Build analyse_B.json content ---\n    analyse = {\n        \"all\":   {\"abs_t_hist_edges_m\": all_edges,  \"abs_t_hist_counts\": all_counts},\n        \"east\":  {\"abs_t_hist_edges_m\": east_edges, \"abs_t_hist_counts\": east_counts,\n                  \"abs_t_peaks_m\": [float(x) for x in east_peaks], \"likely_lane_count\": int(len(east_peaks))},\n        \"west\":  {\"abs_t_hist_edges_m\": west_edges, \"abs_t_hist_counts\": west_counts,\n                  \"abs_t_peaks_m\": [float(x) for x in west_peaks], \"likely_lane_count\": int(len(west_peaks))},\n        \"params\": params_dict(),\n        \"notes\": [\n            \"B1-1/B1-2 方向：南北向，左右按东西侧划分\",\n            \"分界线：PCA-1D + s分箱(17m) + t双峰中点 + 插值平滑\",\n            \"侧别确定：用 +1.75m 偏移与分界线的经度比较，median(lon_offset - lon_divider)>0 则 '+' 为 EAST\",\n            \"直行数估计：在各侧 |t| 分布上做多峰检测，最小峰距≈3.5m；峰数即直行条数的强信号\"\n        ]\n    }\n    return inter, analyse\n\ndef params_dict():\n    return {\n        \"bin_size_m\": BIN_SIZE_M,\n        \"min_pts_per_bin\": MIN_PTS_PER_BIN,\n        \"nbins_t\": NBINS_T,\n        \"min_lane_sep_m\": MIN_LANE_SEP_M,\n        \"sigma_bins\": SIGMA_BINS,\n        \"smooth_win\": SMOOTH_WIN,\n        \"lane_width\": LANE_WIDTH,\n        \"half_lane\": HALF\n    }\n\ndef main():\n    direction_csv = os.path.join(DATA_DIR, \"/home/mw/project/direction.csv\")\n    allowed = load_directions(direction_csv)\n\n    out_inter = {}\n    out_analyse = {}\n\n    for fname, road in INPUT_FILES:\n        path = os.path.join(DATA_DIR, fname)\n        if not os.path.exists(path):\n            out_inter[road] = {\n                \"lane_divider\": [],\n                \"east\": {\"lanes\": {\"east_lane1\": [], \"east_lane2\": []}},\n                \"west\": {\"lanes\": {\"west_lane1\": [], \"west_lane2\": []}}\n            }\n            out_analyse[road] = {\n                \"all\": {\"abs_t_hist_edges_m\": [], \"abs_t_hist_counts\": []},\n                \"east\": {\"abs_t_hist_edges_m\": [], \"abs_t_hist_counts\": [], \"abs_t_peaks_m\": [], \"likely_lane_count\": 0},\n                \"west\": {\"abs_t_hist_edges_m\": [], \"abs_t_hist_counts\": [], \"abs_t_peaks_m\": [], \"likely_lane_count\": 0},\n                \"params\": params_dict(),\n                \"notes\": [\"file not found\"]\n            }\n            continue\n\n        inter, analyse = process_one(path, road, allowed)\n        out_inter[road] = inter\n        out_analyse[road] = analyse\n\n    with open(OUT_INTER, \"w\", encoding=\"utf-8\") as f:\n        json.dump(out_inter, f, ensure_ascii=False, indent=2)\n    with open(OUT_ANALYSE, \"w\", encoding=\"utf-8\") as f:\n        json.dump(out_analyse, f, ensure_ascii=False, indent=2)\n    print(\"Wrote:\", OUT_INTER)\n    print(\"Wrote:\", OUT_ANALYSE)\n\nif __name__ == \"__main__\":\n    main()\n"
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}