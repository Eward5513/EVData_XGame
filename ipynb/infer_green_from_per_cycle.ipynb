{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Auto-generated from `infer_green_from_per_cycle.py`\n\nGenerated on 2025-11-09T22:01:34.\n\nThis notebook was created programmatically to mirror the original Python script.\n"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": "from pathlib import Path\nimport sys\nproject_root = str(Path.cwd().parent.resolve())\nif project_root not in sys.path:\n    sys.path.insert(0, project_root)\n"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\"\"\"\nInfer most likely green intervals per direction (A1/A2/B1/B2)\nfrom per-cycle flow and wait signals using a z-score based likelihood.\n\nInputs (fixed paths):\n  - data/per_cycle_flow.csv\n  - data/per_cycle_wait.csv\n\nOutput:\n  - data/inferred_green.csv with columns:\n      direction,start_offset_s,end_offset_s,start_time,end_time,score\n\nNotes:\n  - Cycle is treated as circular; an interval may wrap past 129 back to 0.\n  - Likelihood uses per-direction z-scores and a wait derivative cue:\n       L = 0.6 * z_flow + 0.2 * ReLU(-(d/dt z_wait)) + 0.2 * (-z_wait)\n\"\"\"\nfrom __future__ import annotations\n\nimport os\nfrom typing import Dict, List, Tuple\n\nimport numpy as np\nimport pandas as pd\n\n\nDIRS: Tuple[str, str, str, str] = (\"A1\", \"A2\", \"B1\", \"B2\")\nFLOW_CSV = os.path.join(\"data\", \"/home/mw/project/per_cycle_flow.csv\")\nWAIT_CSV = os.path.join(\"data\", \"/home/mw/project/per_cycle_wait.csv\")\nOUT_CSV = os.path.join(\"data\", \"/home/mw/project/inferred_green.csv\")\nCYCLE_SECONDS = 130\n\n\ndef _read_per_cycle(path: str) -> pd.DataFrame:\n    if not os.path.exists(path):\n        raise FileNotFoundError(f\"File not found: {path}\")\n    df = pd.read_csv(path)\n    need = {\"offset_s\", \"time\", *DIRS}\n    if not need.issubset(set(df.columns)):\n        missing = sorted(list(need - set(df.columns)))\n        raise ValueError(f\"Missing columns in {path}: {missing}\")\n    # Ensure dense 0..CYCLE_SECONDS-1 offsets\n    df = df.sort_values(\"offset_s\").reset_index(drop=True)\n    return df\n\n\ndef _zscore(x: np.ndarray) -> np.ndarray:\n    if x.size == 0:\n        return x\n    m = float(np.mean(x))\n    s = float(np.std(x))\n    if s <= 0:\n        return np.zeros_like(x, dtype=float)\n    return (x - m) / s\n\n\ndef _derivative_circular(x: np.ndarray) -> np.ndarray:\n    # Central difference with wrap\n    return (np.roll(x, -1) - np.roll(x, 1)) / 2.0\n\n\ndef _smooth_ma(x: np.ndarray, window: int = 5) -> np.ndarray:\n    if window <= 1:\n        return x.astype(float)\n    w = int(window)\n    pad = w // 2\n    # circular pad\n    x_pad = np.r_[x[-pad:], x, x[:pad]]\n    ker = np.ones(w, dtype=float) / w\n    y = np.convolve(x_pad, ker, mode=\"valid\")\n    return y.astype(float)\n\n\ndef _compute_likelihood(flow: np.ndarray, wait: np.ndarray) -> np.ndarray:\n    # Smooth\n    f_s = _smooth_ma(flow, 5)\n    w_s = _smooth_ma(wait, 5)\n    # Z-scores per direction\n    zf = _zscore(f_s)\n    zw = _zscore(w_s)\n    # Wait derivative cue (queue release during green)\n    dw = _derivative_circular(zw)\n    relu_neg_dw = np.clip(-dw, 0.0, None)\n    # Combined score\n    L = 0.6 * zf + 0.2 * relu_neg_dw + 0.2 * (-zw)\n    # Non-negative baseline\n    L = L - min(0.0, float(L.min()))\n    return L\n\n\ndef _segments_from_mask(mask: np.ndarray) -> List[Tuple[int, int]]:\n    \"\"\"\n    Return list of (start, end) inclusive indices for True runs in a circular mask.\n    A segment may wrap (start > end) indicating wraparound.\n    \"\"\"\n    n = len(mask)\n    if n == 0:\n        return []\n    # Trivial cases\n    if np.all(mask):\n        return [(0, n - 1)]\n    if not np.any(mask):\n        return []\n    segs: List[Tuple[int, int]] = []\n    inside = False\n    start = -1\n    for i in range(n):\n        if mask[i] and not inside:\n            inside = True\n            start = i\n        elif not mask[i] and inside:\n            segs.append((start, i - 1))\n            inside = False\n            start = -1\n    if inside:\n        # closes at end; may wrap with beginning\n        if mask[0]:\n            # Merge with head run\n            # Find head run length\n            head_end = 0\n            while head_end < n and mask[head_end]:\n                head_end += 1\n            segs.append((start, (head_end - 1)))  # wrap\n        else:\n            segs.append((start, n - 1))\n    # Merge first and last if both don't wrap and they touch around boundary\n    if len(segs) >= 2 and segs[0][0] == 0 and segs[-1][1] == n - 1:\n        s0, e0 = segs[0]\n        sl, el = segs[-1]\n        segs = [(sl, e0)] + segs[1:-1]\n    return segs\n\n\ndef _score_segment(L: np.ndarray, seg: Tuple[int, int]) -> float:\n    n = len(L)\n    s, e = seg\n    if s <= e:\n        return float(L[s : e + 1].sum())\n    # wrap\n    return float(L[s:].sum() + L[: e + 1].sum())\n\n\ndef _infer_one_direction(flow: np.ndarray, wait: np.ndarray, times: List[str]) -> Tuple[int, int, float, str, str]:\n    \"\"\"\n    Return (start_idx, end_idx, score, start_time, end_time)\n    \"\"\"\n    n = len(flow)\n    L = _compute_likelihood(flow, wait)\n    # Adaptive threshold\n    thr = float(np.quantile(L, 0.70))\n    mask = L >= thr\n    segs = _segments_from_mask(mask)\n    if not segs:\n        # fallback: take top k seconds around the best second\n        best = int(np.argmax(L))\n        k = 8  # ~16s window\n        s = (best - k) % n\n        e = (best + k) % n\n        score = float(L[best])\n        return s, e, score, times[s], times[e]\n    # Score segments by integral\n    seg_scores = [(_score_segment(L, seg), seg) for seg in segs]\n    seg_scores.sort(key=lambda t: t[0], reverse=True)\n    best_score, best_seg = seg_scores[0]\n    s, e = best_seg\n    return s, e, float(best_score), times[s], times[e]\n\n\ndef infer_all() -> pd.DataFrame:\n    fdf = _read_per_cycle(FLOW_CSV)\n    wdf = _read_per_cycle(WAIT_CSV)\n    # Align offsets just in case\n    if not fdf[\"offset_s\"].equals(wdf[\"offset_s\"]):\n        wdf = wdf.set_index(\"offset_s\").reindex(fdf[\"offset_s\"]).reset_index()\n        if \"time\" in wdf.columns and \"time\" in fdf.columns:\n            wdf[\"time\"] = fdf[\"time\"]\n    times = fdf[\"time\"].astype(str).tolist()\n    rows: List[Dict[str, object]] = []\n    for d in DIRS:\n        flow = fdf[d].astype(float).to_numpy()\n        wait = wdf[d].astype(float).to_numpy()\n        s, e, score, ts, te = _infer_one_direction(flow, wait, times)\n        rows.append({\n            \"direction\": d,\n            \"start_offset_s\": int(s),\n            \"end_offset_s\": int(e),\n            \"start_time\": ts,\n            \"end_time\": te,\n            \"score\": float(round(score, 6)),\n        })\n    return pd.DataFrame(rows)\n\n\ndef main() -> int:\n    out = infer_all()\n    out.to_csv(OUT_CSV, index=False)\n    print(f\"Wrote inferred intervals to {OUT_CSV}\")\n    for _, r in out.iterrows():\n        print(f\"- {r['direction']}: [{r['start_offset_s']}, {r['end_offset_s']}]  ({r['start_time']} ~ {r['end_time']})  score={r['score']}\")\n    return 0\n\n\nif __name__ == \"__main__\":\n    raise SystemExit(main())\n\n\n"
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}